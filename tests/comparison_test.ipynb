{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison test\n",
    "\n",
    "This test compares the output from  `run_classifier.py` in the huggingface port to `bert_sklearn` on a small test subset from sst-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `run_classifier.py` from huggingface port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/17/2019 00:07:50 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/17/2019 00:07:51 - INFO - bert_sklearn.model.pytorch_pretrained.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "05/17/2019 00:07:51 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "05/17/2019 00:07:51 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmprqyqpefb\n",
      "05/17/2019 00:07:54 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/17/2019 00:07:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "05/17/2019 00:07:57 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   Writing example 0 of 200\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   guid: train-1\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   tokens: [CLS] hide new secret ##ions from the parental units [SEP]\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_ids: 101 5342 2047 3595 8496 2013 1996 18643 3197 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   label: 0 (id = 0)\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   guid: train-2\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   tokens: [CLS] contains no wit , only labor ##ed gag ##s [SEP]\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_ids: 101 3397 2053 15966 1010 2069 4450 2098 18201 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   label: 0 (id = 0)\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   guid: train-3\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   tokens: [CLS] that loves its characters and communicate ##s something rather beautiful about human nature [SEP]\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_ids: 101 2008 7459 2049 3494 1998 10639 2015 2242 2738 3376 2055 2529 3267 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   label: 1 (id = 1)\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   guid: train-4\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   tokens: [CLS] remains utterly satisfied to remain the same throughout [SEP]\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_ids: 101 3464 12580 8510 2000 3961 1996 2168 2802 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   label: 0 (id = 0)\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   guid: train-5\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   tokens: [CLS] on the worst revenge - of - the - ne ##rds cl ##iche ##s the filmmakers could dr ##edge up [SEP]\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_ids: 101 2006 1996 5409 7195 1011 1997 1011 1996 1011 11265 17811 18856 17322 2015 1996 16587 2071 2852 24225 2039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   label: 0 (id = 0)\n",
      "05/17/2019 00:07:59 - INFO - __main__ -   ***** Running training *****\n",
      "05/17/2019 00:07:59 - INFO - __main__ -     Num examples = 200\n",
      "05/17/2019 00:07:59 - INFO - __main__ -     Batch size = 16\n",
      "05/17/2019 00:07:59 - INFO - __main__ -     Num steps = 24\n",
      "\r",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\r",
      "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   8%|▊         | 1/13 [00:00<00:03,  3.29it/s]\u001b[A\n",
      "\r",
      "Iteration:  15%|█▌        | 2/13 [00:00<00:03,  3.38it/s]\u001b[A\n",
      "\r",
      "Iteration:  23%|██▎       | 3/13 [00:00<00:02,  3.48it/s]\u001b[A\n",
      "\r",
      "Iteration:  31%|███       | 4/13 [00:01<00:02,  3.57it/s]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 5/13 [00:01<00:02,  3.63it/s]\u001b[A\n",
      "\r",
      "Iteration:  46%|████▌     | 6/13 [00:01<00:01,  3.67it/s]\u001b[A\n",
      "\r",
      "Iteration:  54%|█████▍    | 7/13 [00:01<00:01,  3.70it/s]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▏   | 8/13 [00:02<00:01,  3.73it/s]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 9/13 [00:02<00:01,  3.74it/s]\u001b[A\n",
      "\r",
      "Iteration:  77%|███████▋  | 10/13 [00:02<00:00,  3.75it/s]\u001b[A\n",
      "\r",
      "Iteration:  85%|████████▍ | 11/13 [00:02<00:00,  3.76it/s]\u001b[A\n",
      "\r",
      "Iteration:  92%|█████████▏| 12/13 [00:03<00:00,  3.77it/s]\u001b[A\n",
      "\r",
      "Iteration: 100%|██████████| 13/13 [00:03<00:00,  4.11it/s]\u001b[A\n",
      "\u001b[A\r",
      "Epoch:  50%|█████     | 1/2 [00:03<00:03,  3.42s/it]\n",
      "\r",
      "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\r",
      "Iteration:   8%|▊         | 1/13 [00:00<00:03,  3.84it/s]\u001b[A\n",
      "\r",
      "Iteration:  15%|█▌        | 2/13 [00:00<00:02,  3.82it/s]\u001b[A\n",
      "\r",
      "Iteration:  23%|██▎       | 3/13 [00:00<00:02,  3.80it/s]\u001b[A\n",
      "\r",
      "Iteration:  31%|███       | 4/13 [00:01<00:02,  3.80it/s]\u001b[A\n",
      "\r",
      "Iteration:  38%|███▊      | 5/13 [00:01<00:02,  3.78it/s]\u001b[A\n",
      "\r",
      "Iteration:  46%|████▌     | 6/13 [00:01<00:01,  3.77it/s]\u001b[A\n",
      "\r",
      "Iteration:  54%|█████▍    | 7/13 [00:01<00:01,  3.75it/s]\u001b[A\n",
      "\r",
      "Iteration:  62%|██████▏   | 8/13 [00:02<00:01,  3.76it/s]\u001b[A\n",
      "\r",
      "Iteration:  69%|██████▉   | 9/13 [00:02<00:01,  3.77it/s]\u001b[A\n",
      "\r",
      "Iteration:  77%|███████▋  | 10/13 [00:02<00:00,  3.78it/s]\u001b[A\n",
      "\r",
      "Iteration:  85%|████████▍ | 11/13 [00:02<00:00,  3.78it/s]\u001b[A\n",
      "\r",
      "Iteration:  92%|█████████▏| 12/13 [00:03<00:00,  3.77it/s]\u001b[A05/17/2019 00:08:06 - WARNING - bert_sklearn.model.pytorch_pretrained.optimization -   Training beyond specified 't_total' steps with schedule 'warmup_linear'. Learning rate set to 0.0. Please set 't_total' of BertAdam correctly.\n",
      "\n",
      "\r",
      "Iteration: 100%|██████████| 13/13 [00:03<00:00,  4.11it/s]\u001b[A\n",
      "\u001b[A\r",
      "Epoch: 100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   Writing example 0 of 100\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   guid: dev-1\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   tokens: [CLS] it ' s a charming and often affecting journey . [SEP]\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   label: 1 (id = 1)\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   guid: dev-2\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   tokens: [CLS] un ##fl ##in ##ching ##ly bleak and desperate [SEP]\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_ids: 101 4895 10258 2378 8450 2135 21657 1998 7143 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   label: 0 (id = 0)\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   guid: dev-3\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   tokens: [CLS] allows us to hope that nolan is poised to embark a major career as a commercial yet in ##vent ##ive filmmaker . [SEP]\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_ids: 101 4473 2149 2000 3246 2008 13401 2003 22303 2000 28866 1037 2350 2476 2004 1037 3293 2664 1999 15338 3512 12127 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   label: 1 (id = 1)\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   guid: dev-4\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   tokens: [CLS] the acting , costumes , music , cinematography and sound are all as ##tou ##nding given the production ' s aus ##ter ##e local ##es . [SEP]\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_ids: 101 1996 3772 1010 12703 1010 2189 1010 16434 1998 2614 2024 2035 2004 24826 15683 2445 1996 2537 1005 1055 17151 3334 2063 2334 2229 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   label: 1 (id = 1)\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   *** Example ***\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   guid: dev-5\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   tokens: [CLS] it ' s slow - - very , very slow . [SEP]\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_ids: 101 2009 1005 1055 4030 1011 1011 2200 1010 2200 4030 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   label: 0 (id = 0)\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "05/17/2019 00:08:06 - INFO - __main__ -     Num examples = 100\n",
      "05/17/2019 00:08:06 - INFO - __main__ -     Batch size = 8\n",
      "\r",
      "Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]\r",
      "Evaluating:  31%|███       | 4/13 [00:00<00:00, 31.50it/s]\r",
      "Evaluating:  62%|██████▏   | 8/13 [00:00<00:00, 31.30it/s]\r",
      "Evaluating:  92%|█████████▏| 12/13 [00:00<00:00, 31.25it/s]\r",
      "Evaluating: 100%|██████████| 13/13 [00:00<00:00, 32.23it/s]\n",
      "05/17/2019 00:08:06 - INFO - __main__ -   ***** Eval results *****\n",
      "05/17/2019 00:08:06 - INFO - __main__ -     acc = 0.81\n",
      "05/17/2019 00:08:06 - INFO - __main__ -     eval_loss = 0.42863699048757553\n",
      "05/17/2019 00:08:06 - INFO - __main__ -     global_step = 26\n",
      "05/17/2019 00:08:06 - INFO - __main__ -     loss = 0.3171330988407135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "cd ..\n",
    "python ./tests/run_classifier.py --task_name sst-2 \\\n",
    "                            --data_dir ./tests/data/sst2 \\\n",
    "                            --do_train  --do_eval \\\n",
    "                            --output_dir ./tests \\\n",
    "                            --bert_model bert-base-uncased \\\n",
    "                            --do_lower_case \\\n",
    "                            --learning_rate 5e-5 \\\n",
    "                            --gradient_accumulation_steps 1 \\\n",
    "                            --max_seq_length 64 \\\n",
    "                            --train_batch_size 16 \\\n",
    "                            --eval_batch_size 8 \\\n",
    "                            --num_train_epochs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.81\n",
      "eval_loss = 0.42863699048757553\n",
      "global_step = 26\n",
      "loss = 0.3171330988407135\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat eval_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `bert_sklearn` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST-2 train data size: 200 \n",
      "SST-2 dev data size: 100 \n",
      "Building sklearn text classifier...\n",
      "Loading bert-base-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "train data size: 200, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:03<00:00,  3.90it/s, loss=0.673]\n",
      "Training: 100%|██████████| 13/13 [00:03<00:00,  3.93it/s, loss=0.317]\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.4286, Accuracy: 81.00%\n",
      "CPU times: user 13.2 s, sys: 3.84 s, total: 17 s\n",
      "Wall time: 18.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sys.path.append(\"../\") \n",
    "from bert_sklearn import BertClassifier\n",
    "from bert_sklearn import load_model\n",
    "\n",
    "\n",
    "def get_sst_test_data(train_file ='./data/sst2/train.tsv',\n",
    "                dev_file  = './data/sst2/dev.tsv'):\n",
    "    \n",
    "    train = pd.read_csv(train_file, sep='\\t', encoding='utf8', keep_default_na=False)\n",
    "    train.columns=['text','label']\n",
    "    print(\"SST-2 train data size: %d \"%(len(train)))\n",
    "    \n",
    "    dev = pd.read_csv(dev_file, sep='\\t', encoding='utf8', keep_default_na=False)\n",
    "    dev.columns=['text','label']\n",
    "    print(\"SST-2 dev data size: %d \"%(len(dev)))\n",
    "    label_list = np.unique(train['label'])\n",
    "\n",
    "    X_train = train['text']\n",
    "    y_train = train['label']\n",
    "    X_dev = dev['text']\n",
    "    y_dev = dev['label']\n",
    "\n",
    "    return X_train,y_train, X_dev, y_dev\n",
    "\n",
    "\n",
    "X_train,y_train, X_dev, y_dev =  get_sst_test_data()\n",
    "\n",
    "# define model\n",
    "model = BertClassifier()\n",
    "model.validation_fraction = 0.0\n",
    "model.learning_rate = 5e-5 \n",
    "model.gradient_accumulation_steps = 1\n",
    "model.max_seq_length = 64\n",
    "model.train_batch_size = 16\n",
    "model.eval_batch_size=8\n",
    "model.epochs = 2\n",
    "\n",
    "# fit\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# score\n",
    "accy = model.score(X_dev,y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm bert_sklearn.log\n",
    "!rm  eval_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
